{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktxdLosxtgZd",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Light GCN * 2 Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjmfT8zVf1yt"
      },
      "source": [
        "## 0. Env Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqljF7Lmf1yu"
      },
      "source": [
        "### 0.1 Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T04:05:08.204448Z",
          "iopub.status.busy": "2022-09-28T04:05:08.204127Z",
          "iopub.status.idle": "2022-09-28T04:05:08.208400Z",
          "shell.execute_reply": "2022-09-28T04:05:08.207456Z",
          "shell.execute_reply.started": "2022-09-28T04:05:08.204422Z"
        },
        "id": "-Cvheinkf1yu"
      },
      "outputs": [],
      "source": [
        "!pip install ipywidgets\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.12.0+cu116.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.12.0+cu116.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3WDfWS2f1yv"
      },
      "source": [
        "### 0.2 Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T04:05:16.755256Z",
          "iopub.status.busy": "2022-09-28T04:05:16.754944Z",
          "iopub.status.idle": "2022-09-28T04:05:19.559673Z",
          "shell.execute_reply": "2022-09-28T04:05:19.558872Z",
          "shell.execute_reply.started": "2022-09-28T04:05:16.755232Z"
        },
        "id": "Y9fonQcxt3do",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torch_geometric\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.utils import degree\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzLUutf7uNAS",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Check our environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T04:05:24.540932Z",
          "iopub.status.busy": "2022-09-28T04:05:24.540440Z",
          "iopub.status.idle": "2022-09-28T04:05:24.551815Z",
          "shell.execute_reply": "2022-09-28T04:05:24.550941Z",
          "shell.execute_reply.started": "2022-09-28T04:05:24.540906Z"
        },
        "id": "J_CDy1cbuF4_",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "torch_geometric.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQDvc6usf1yw"
      },
      "source": [
        "Set device for torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T04:05:28.094312Z",
          "iopub.status.busy": "2022-09-28T04:05:28.093946Z",
          "iopub.status.idle": "2022-09-28T04:05:28.098528Z",
          "shell.execute_reply": "2022-09-28T04:05:28.097844Z",
          "shell.execute_reply.started": "2022-09-28T04:05:28.094287Z"
        },
        "id": "b4pKT5jUt3pz",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFdaSRE5f1yw"
      },
      "source": [
        "## 1. Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYnQc9UH07Fg",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### 1.1 Read Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to access the Google Drive, first go to: https://drive.google.com/drive/folders/1TzyQFgm_szZMo6d6RyrwPE4Hkc_0I0og?usp=sharing, then go to \"Shared with me\", right click on the folder and select \"Add a shortcut to Drive\"."
      ],
      "metadata": {
        "id": "wxmrfdLlLZQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "2nKt-DY7QCSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T04:19:00.774144Z",
          "iopub.status.busy": "2022-09-28T04:19:00.773432Z",
          "iopub.status.idle": "2022-09-28T04:19:03.716543Z",
          "shell.execute_reply": "2022-09-28T04:19:03.715618Z",
          "shell.execute_reply.started": "2022-09-28T04:19:00.774118Z"
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "uGkiEskGf1yx"
      },
      "outputs": [],
      "source": [
        "columns_name = ['place_index', 'user_index', 'rating']\n",
        "review_df_1 = pd.read_csv(\"/content/drive/MyDrive/cities/toronto/train.tsv\", sep=\"\\t\")[columns_name].astype(int)\n",
        "review_df_2 = pd.read_csv(\"/content/drive/MyDrive/cities/Los_angeles/train.tsv\", sep=\"\\t\")[columns_name].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxw2vEInf1yx"
      },
      "source": [
        "### 1.2 Number of User and Place"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0zgWkQAf1yx"
      },
      "outputs": [],
      "source": [
        "max_user_id_1 = review_df_1['user_index'].max()\n",
        "max_place_id_1 = review_df_1['place_index'].max()\n",
        "print(max_user_id_1)\n",
        "print(max_place_id_1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_user_id_2 = review_df_2['user_index'].max()\n",
        "max_place_id_2 = review_df_2['place_index'].max()\n",
        "print(max_user_id_2)\n",
        "print(max_place_id_2)"
      ],
      "metadata": {
        "id": "2s9fnn9bRCZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_node_id_1 = max_user_id_1 + max_place_id_1 + 1 # since place_id starts from 0\n",
        "print(max_node_id_1)"
      ],
      "metadata": {
        "id": "sHAliWim8k7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_node_id_2 = max_user_id_2 + max_place_id_2 + 1 # since place_id starts from 0\n",
        "print(max_node_id_2)"
      ],
      "metadata": {
        "id": "mYi7GsGfRVB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrPQNnUjf1yx"
      },
      "source": [
        "### 1.3 Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T04:19:08.987210Z",
          "iopub.status.busy": "2022-09-28T04:19:08.986887Z",
          "iopub.status.idle": "2022-09-28T04:19:09.175583Z",
          "shell.execute_reply": "2022-09-28T04:19:09.174815Z",
          "shell.execute_reply.started": "2022-09-28T04:19:08.987185Z"
        },
        "id": "SWwvL8JOmrT4",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# It is possible some users or restaurants in test do not exist in \n",
        "# train after this split.\n",
        "# Will need to retrain the model with combined train+test after picking\n",
        "# the best model structure\n",
        "train_1, test_1 = train_test_split(review_df_1.values, test_size=0.1)\n",
        "train_df_1 = pd.DataFrame(train_1, columns=review_df_1.columns)\n",
        "test_df_1 = pd.DataFrame(test_1, columns=review_df_1.columns)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_2, test_2 = train_test_split(review_df_2.values, test_size=0.1)\n",
        "train_df_2 = pd.DataFrame(train_2, columns=review_df_2.columns)\n",
        "test_df_2 = pd.DataFrame(test_2, columns=review_df_2.columns)"
      ],
      "metadata": {
        "id": "EWdBbXB-RtGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6GlUta4f1yx"
      },
      "source": [
        "### 1.4 Get distribution of different ratings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdgnxzLWf1yy"
      },
      "outputs": [],
      "source": [
        "# Weights will be used to normalize loss function\n",
        "def get_weights(df):\n",
        "    rating_counts = np.array([len(df[df['rating'] == i]) for i in [1, 2, 3, 4, 5]])\n",
        "    inverse_count = 1 / rating_counts\n",
        "    norm = np.linalg.norm(inverse_count)\n",
        "    normalized_inverse_count = inverse_count / norm\n",
        "\n",
        "    return normalized_inverse_count"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights_1 = get_weights(train_df_1)\n",
        "print(weights_1)"
      ],
      "metadata": {
        "id": "SWLbfguSSB8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights_2 = get_weights(train_df_2)\n",
        "print(weights_2)"
      ],
      "metadata": {
        "id": "uyRf6RQLSBs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_ens4xxf1yy"
      },
      "outputs": [],
      "source": [
        "train_df_1['weight'] = train_df_1['rating'].map(lambda val: weights_1[int(val)-1])\n",
        "test_df_1['weight'] = test_df_1['rating'].map(lambda val: weights_1[int(val)-1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_2['weight'] = train_df_2['rating'].map(lambda val: weights_2[int(val)-1])\n",
        "test_df_2['weight'] = test_df_2['rating'].map(lambda val: weights_2[int(val)-1])"
      ],
      "metadata": {
        "id": "__t8lfwhSOqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubeyUQ3Vf1yy"
      },
      "outputs": [],
      "source": [
        "# Check data snippet\n",
        "train_df_1.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_2.head(5)"
      ],
      "metadata": {
        "id": "KEh5PQWPSeZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNoblY5kxlv_",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### 1.5 Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T04:29:41.367161Z",
          "iopub.status.busy": "2022-09-28T04:29:41.366316Z",
          "iopub.status.idle": "2022-09-28T04:29:41.372542Z",
          "shell.execute_reply": "2022-09-28T04:29:41.371543Z",
          "shell.execute_reply.started": "2022-09-28T04:29:41.367132Z"
        },
        "id": "NQRGy-CJnOkg",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data.to_numpy()\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index, 0].astype(np.compat.long), \\\n",
        "            self.data[index, 1].astype(np.compat.long), \\\n",
        "            self.data[index, 2:3].astype(np.float32), \\\n",
        "            self.data[index, 3]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjHZg1Eu-MKs",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## 2. Graph Construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T04:29:45.034832Z",
          "iopub.status.busy": "2022-09-28T04:29:45.034193Z",
          "iopub.status.idle": "2022-09-28T04:29:45.088226Z",
          "shell.execute_reply": "2022-09-28T04:29:45.087347Z",
          "shell.execute_reply.started": "2022-09-28T04:29:45.034807Z"
        },
        "id": "O3BkGyV9pkce",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "u_t_1 = torch.LongTensor(train_df_1.user_index.to_numpy())\n",
        "p_t_1 = torch.LongTensor(train_df_1.place_index.to_numpy()) + max_user_id_1 + 1\n",
        "\n",
        "train_edge_index_1 = torch.stack((torch.cat([u_t_1, p_t_1]),torch.cat([p_t_1, u_t_1]))).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "u_t_2 = torch.LongTensor(train_df_2.user_index.to_numpy())\n",
        "p_t_2 = torch.LongTensor(train_df_2.place_index.to_numpy()) + max_user_id_2 + 1\n",
        "\n",
        "train_edge_index_2 = torch.stack((torch.cat([u_t_2, p_t_2]),torch.cat([p_t_2, u_t_2]))).to(device)"
      ],
      "metadata": {
        "id": "WORKet3XSsIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGlRLHR_f1yz"
      },
      "outputs": [],
      "source": [
        "train_df_1['place_index'] = train_df_1['place_index'] + max_user_id_1 + 1\n",
        "test_df_1['place_index'] = test_df_1['place_index'] + max_user_id_1 + 1\n",
        "# assert that there's no index overlapping\n",
        "intersection_1 = set(train_df_1['place_index'].unique()).intersection(set(train_df_1['user_index'].unique()))\n",
        "assert len(intersection_1) == 0\n",
        "\n",
        "intersection_1 = set(test_df_1['place_index'].unique()).intersection(set(test_df_1['user_index'].unique()))\n",
        "assert len(intersection_1) == 0\n",
        "\n",
        "train_dataset_1 = MyDataset(train_df_1)\n",
        "test_dataset_1 = MyDataset(test_df_1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_2['place_index'] = train_df_2['place_index'] + max_user_id_2 + 1\n",
        "test_df_2['place_index'] = test_df_2['place_index'] + max_user_id_2 + 1\n",
        "# assert that there's no index overlapping\n",
        "intersection_2 = set(train_df_2['place_index'].unique()).intersection(set(train_df_2['user_index'].unique()))\n",
        "assert len(intersection_2) == 0\n",
        "\n",
        "intersection_2 = set(test_df_2['place_index'].unique()).intersection(set(test_df_2['user_index'].unique()))\n",
        "assert len(intersection_2) == 0\n",
        "\n",
        "train_dataset_2 = MyDataset(train_df_2)\n",
        "test_dataset_2 = MyDataset(test_df_2)"
      ],
      "metadata": {
        "id": "E6I6ZyCpTKIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ys1P7mtcr54",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## 3. Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49WD8SryyUds",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### 3.1 LightGCN Convolutional Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T04:29:49.256735Z",
          "iopub.status.busy": "2022-09-28T04:29:49.256047Z",
          "iopub.status.idle": "2022-09-28T04:29:49.262307Z",
          "shell.execute_reply": "2022-09-28T04:29:49.261567Z",
          "shell.execute_reply.started": "2022-09-28T04:29:49.256707Z"
        },
        "id": "-aTMoHisNIh_",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "class LightGCNConv(MessagePassing):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(aggr='add')\n",
        "\n",
        "    def forward(self, x, edge_index, num_nodes):\n",
        "        # Compute normalization\n",
        "        from_, to_ = edge_index\n",
        "        deg = degree(to_, x.size(0), dtype=x.dtype)\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "        norm = deg_inv_sqrt[from_] * deg_inv_sqrt[to_]\n",
        "        # Start propagating messages (no update after aggregation)\n",
        "        return self.propagate(edge_index, x=x, norm=norm)\n",
        "\n",
        "    def message(self, x_j, norm):\n",
        "        return norm.view(-1, 1) * x_j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2o-hw1of1y0"
      },
      "outputs": [],
      "source": [
        "# Initialize node embeddings as one-hot embeddings\n",
        "test_x = torch.Tensor(np.eye(5))\n",
        "\n",
        "# Construct edges\n",
        "test_edge_index = torch.LongTensor(np.array([\n",
        "  [0, 0, 1, 1, 2, 3, 3, 4],\n",
        "  [2, 3, 3, 4, 0, 0, 1, 1]\n",
        "]))\n",
        "\n",
        "# Check out the result of passing the embeddings through our Graph Convolutional Network\n",
        "LightGCNConv()(test_x, test_edge_index, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2tW9FJFqNjn",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### 3.2 Recommender System GNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T04:29:55.128388Z",
          "iopub.status.busy": "2022-09-28T04:29:55.128015Z",
          "iopub.status.idle": "2022-09-28T04:29:55.137444Z",
          "shell.execute_reply": "2022-09-28T04:29:55.136459Z",
          "shell.execute_reply.started": "2022-09-28T04:29:55.128356Z"
        },
        "id": "nT5LTkI8Ml1c",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "class LightGCN(nn.Module):\n",
        "    def __init__(self, latent_dim, num_layers, max_index):\n",
        "        super(LightGCN, self).__init__()\n",
        "        self.embedding = nn.Embedding(max_index, latent_dim)\n",
        "        self.convs = nn.ModuleList(LightGCNConv() for _ in range(num_layers))\n",
        "        self.init_parameters()\n",
        "        self.nn = nn.Linear(2*latent_dim, 1)\n",
        "\n",
        "        self.max_index = max_index\n",
        "\n",
        "    def init_parameters(self):\n",
        "        nn.init.normal_(self.embedding.weight, std=0.1) \n",
        "\n",
        "    def forward(self, edge_index):\n",
        "        emb0 = self.embedding.weight\n",
        "        embs = [emb0]\n",
        "        emb = emb0\n",
        "        for conv in self.convs:\n",
        "            emb = conv(x=emb, edge_index=edge_index, num_nodes=self.max_index)\n",
        "            embs.append(emb)\n",
        "\n",
        "        out = torch.mean(torch.stack(embs, dim=0), dim=0)\n",
        "        return emb0, out\n",
        "    \n",
        "    def pred(self, users, items, embeddings):\n",
        "        user_emb = embeddings[users]\n",
        "        item_emb = embeddings[items]\n",
        "        x = torch.cat((user_emb,item_emb), 1)\n",
        "        x = self.nn(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qOC3fF9m6cH",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## 4. Train and evaluate models\n",
        "\n",
        "Set Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T04:29:59.336359Z",
          "iopub.status.busy": "2022-09-28T04:29:59.335998Z",
          "iopub.status.idle": "2022-09-28T04:29:59.340947Z",
          "shell.execute_reply": "2022-09-28T04:29:59.340144Z",
          "shell.execute_reply.started": "2022-09-28T04:29:59.336319Z"
        },
        "id": "MZtgfxxIm5nL",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "latent_dim = 64\n",
        "n_layers = 3 \n",
        "\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 100\n",
        "DECAY = 0.0003\n",
        "LR = 0.0005\n",
        "K = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmN76R38f1y0"
      },
      "source": [
        "Set Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T04:30:01.615743Z",
          "iopub.status.busy": "2022-09-28T04:30:01.615380Z",
          "iopub.status.idle": "2022-09-28T04:30:01.696745Z",
          "shell.execute_reply": "2022-09-28T04:30:01.695686Z",
          "shell.execute_reply.started": "2022-09-28T04:30:01.615718Z"
        },
        "id": "JgZrgHUnf1y0"
      },
      "outputs": [],
      "source": [
        "lightgcn_1 = LightGCN(\n",
        "    latent_dim=latent_dim,\n",
        "    num_layers=n_layers,\n",
        "    max_index=max_node_id_1 + 1\n",
        ")\n",
        "lightgcn_1 = lightgcn_1.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lightgcn_2 = LightGCN(\n",
        "    latent_dim=latent_dim,\n",
        "    num_layers=n_layers,\n",
        "    max_index=max_node_id_2 + 1\n",
        ")\n",
        "lightgcn_2 = lightgcn_2.to(device)"
      ],
      "metadata": {
        "id": "g0i9hXaIUJCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a model for the shared layers"
      ],
      "metadata": {
        "id": "xdS9oUezi6MX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w93IgMOUi37z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpTCkdS6f1y0"
      },
      "source": [
        "Set Train Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T04:32:32.252808Z",
          "iopub.status.busy": "2022-09-28T04:32:32.252472Z",
          "iopub.status.idle": "2022-09-28T04:32:32.259687Z",
          "shell.execute_reply": "2022-09-28T04:32:32.258563Z",
          "shell.execute_reply.started": "2022-09-28T04:32:32.252782Z"
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "kHSIkLMuf1y0"
      },
      "outputs": [],
      "source": [
        "def get_testset_loss(model_1, testset_1, embeddings_1, model_2, testset_2, embeddings_2, loss_fn):\n",
        "    loss_list = []\n",
        "    model_1.eval()\n",
        "    model_2.eval()\n",
        "    with torch.no_grad():\n",
        "        test_dataloader_1 = DataLoader(testset_1, batch_size=BATCH_SIZE)\n",
        "        test_dataloader_2 = DataLoader(testset_2, batch_size=BATCH_SIZE)\n",
        "        for model_1_data, model_2_data in tuple(zip(test_dataloader_1, test_dataloader_2)):\n",
        "            items_1, users_1, ratings_1, weights_1 = model_1_data\n",
        "            items_2, users_2, ratings_2, weights_2 = model_2_data\n",
        "            users_1, items_1, ratings_1, weights_1 = users_1.to(device), items_1.to(device), ratings_1.to(device), weights_1.to(device)\n",
        "            users_2, items_2, ratings_2, weights_2 = users_2.to(device), items_2.to(device), ratings_2.to(device), weights_2.to(device)\n",
        "            pred_1 = model_1.pred(users_1, items_1, embeddings_1)\n",
        "            pred_2 = model_2.pred(users_2, items_2, embeddings_2)\n",
        "            loss = loss_fn(pred_1, ratings_1, weights_1, pred_2, ratings_2, weights_2, 0.5)\n",
        "            \n",
        "            loss_list.append(loss.item())\n",
        "            \n",
        "    return sum(loss_list) / len(loss_list)\n",
        "\n",
        "\n",
        "def train(model_1, optimizer_1, train_dataset_1, test_dataset_1, train_edge_index_1, model_2, optimizer_2, train_dataset_2, test_dataset_2, train_edge_index_2, loss_fn):\n",
        "    loss_list_epoch = []\n",
        "    valid_loss_list_epoch = []\n",
        "    train_dataloader_1 = DataLoader(train_dataset_1, batch_size=BATCH_SIZE)\n",
        "    train_dataloader_2 = DataLoader(train_dataset_2, batch_size=BATCH_SIZE)\n",
        "    min_valid_loss = None\n",
        "    min_loss_model = None\n",
        "    for epoch in tqdm(range(EPOCHS)):\n",
        "        n_batch_1 = int(len(train_dataset_1)/BATCH_SIZE)\n",
        "        n_batch_2 = int(len(train_dataset_2)/BATCH_SIZE)\n",
        "        loss_list = []\n",
        "        model_1.train()\n",
        "        model_2.train()\n",
        "        for model_1_data, model_2_data in tqdm(tuple(zip(train_dataloader_1, train_dataloader_2))):\n",
        "            items_1, users_1, ratings_1, weights_1 = model_1_data\n",
        "            items_2, users_2, ratings_2, weights_2 = model_2_data\n",
        "            optimizer_1.zero_grad()\n",
        "            optimizer_2.zero_grad()\n",
        "            users_1, items_1, ratings_1, weights_1 = users_1.to(device), items_1.to(device), ratings_1.to(device), weights_1.to(device)\n",
        "            users_2, items_2, ratings_2, weights_2 = users_2.to(device), items_2.to(device), ratings_2.to(device), weights_2.to(device)\n",
        "            _, embeddings_1 = model_1(train_edge_index_1)\n",
        "            _, embeddings_2 = model_2(train_edge_index_2)\n",
        "            pred_1 = model_1.pred(users_1, items_1, embeddings_1)\n",
        "            pred_2 = model_2.pred(users_2, items_2, embeddings_2)\n",
        "            loss = loss_fn(pred_1, ratings_1, weights_1, pred_2, ratings_2, weights_2, 0.5)\n",
        "            loss.backward()\n",
        "            optimizer_1.step()\n",
        "            optimizer_2.step()\n",
        "            loss_list.append(loss.item())\n",
        "            \n",
        "        # evaluate on validation data\n",
        "        valid_loss = get_testset_loss(model_1, test_dataset_1, embeddings_1, model_2, test_dataset_2, embeddings_2, loss_fn)\n",
        "        if min_valid_loss is None or valid_loss < min_valid_loss:\n",
        "            min_valid_loss = valid_loss\n",
        "            min_loss_model_1 = torch.save(model_1.state_dict(), f\"model_1_epoch_{epoch}.ckpt\")\n",
        "            min_loss_model_2 = torch.save(model_2.state_dict(), f\"model_2_epoch_{epoch}.ckpt\")\n",
        "            \n",
        "        valid_loss_list_epoch.append(round(valid_loss, 4))\n",
        "        loss_list_epoch.append(round(np.mean(loss_list),4))\n",
        "\n",
        "    return loss_list_epoch, valid_loss_list_epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4xJSiBiznki",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Set Loss and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T04:34:12.752383Z",
          "iopub.status.busy": "2022-09-28T04:34:12.752022Z",
          "iopub.status.idle": "2022-09-28T04:34:12.757327Z",
          "shell.execute_reply": "2022-09-28T04:34:12.756348Z",
          "shell.execute_reply.started": "2022-09-28T04:34:12.752357Z"
        },
        "id": "eKBv9eXongux",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Calculate weights of different labels and define weighted MSE loss\n",
        "def weighted_MSE(preds_1, targets_1, weights_1, preds_2, targets_2, weights_2, l):\n",
        "    loss_1 = (weights_1 * (preds_1 - targets_1) ** 2).mean()\n",
        "    loss_2 = (weights_2 * (preds_2 - targets_2) ** 2).mean()\n",
        "    return l * loss_1 + (1 - l) * loss_2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = weighted_MSE\n",
        "optimizer_1 = torch.optim.Adam(lightgcn_1.parameters(), lr=LR)\n",
        "optimizer_2 = torch.optim.Adam(lightgcn_2.parameters(), lr=LR)"
      ],
      "metadata": {
        "id": "ozrSatumU119"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTU8VBLSf1y1"
      },
      "source": [
        "Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T04:34:15.782594Z",
          "iopub.status.busy": "2022-09-28T04:34:15.782236Z",
          "iopub.status.idle": "2022-09-28T04:34:52.217021Z",
          "shell.execute_reply": "2022-09-28T04:34:52.214527Z",
          "shell.execute_reply.started": "2022-09-28T04:34:15.782569Z"
        },
        "id": "iXfsuJlcy3FT",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "loss_history, valid_loss_history = train(lightgcn_1, optimizer_1, train_dataset_1, test_dataset_1, train_edge_index_1, lightgcn_2, optimizer_2, train_dataset_2, test_dataset_2, train_edge_index_2, loss_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHFObo9Mf1y1"
      },
      "source": [
        "Plot the Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-27T22:39:49.754650Z",
          "iopub.status.busy": "2022-09-27T22:39:49.753740Z",
          "iopub.status.idle": "2022-09-27T22:39:49.963771Z",
          "shell.execute_reply": "2022-09-27T22:39:49.962504Z",
          "shell.execute_reply.started": "2022-09-27T22:39:49.754650Z"
        },
        "id": "Z5P2Zf6yT4Uu",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "epoch_list = [(i+1) for i in range(EPOCHS)]\n",
        "\n",
        "plt.plot(epoch_list, loss_history, label='Training Loss')\n",
        "plt.plot(epoch_list, valid_loss_history, label='Validation Loss')\n",
        "with open('/content/drive/MyDrive/CS330_Project/mt(joint_loss)_cs330_light_gcn_lr=0.0005.txt', 'a+') as fp:\n",
        "  for i in range(len(epoch_list)):\n",
        "    fp.write(\"%s %s %s\\n\" % (epoch_list[i], loss_history[i], valid_loss_history[i]))\n",
        "\n",
        "print('loss history', loss_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BBdAXZ4EEKkd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "I2tW9FJFqNjn"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}